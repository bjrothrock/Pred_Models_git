xlabel <- as.expression(expression( paste("Radius (", R[500], ")") ))
ylabel <- "Scaled Temperature"
p <- ggplot(data=A, aes(x=r.r500, y=sckT, ymin=sckT.lo, ymax=sckT.up, fill=type, linetype=type)) +
geom_line() +
geom_ribbon(alpha=0.5) +
scale_x_log10() +
scale_y_log10() +
xlab(xlabel) +
ylab(ylabel)
ggsave(p, file="CC-vs_nCC_kT_prof.pdf", width=8, height=4.5)
plot(p)
library(ggmap)
qmap(location = "boston university")
install.packages("ggrepel")
library(ggmap)
library(ggplot2)
qmap(location = "boston university")
library("ggrepel", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
qmap(location = "boston university")
devtools::install_github('hadley/ggplot2')
devtools::install_github('thomasp85/ggforce')
devtools::install_github('thomasp85/ggraph')
devtools::install_github('slowkow/ggrepel')
library("ggraph", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
qmap(location = "boston university")
2+2
x<=2+2
x<-2+2
x
y<-rnorm(100)
y
plot(y)
hist(y)
z<-rexp(100)
z
scatter(x,z)
plot(x,z)
plot(y,z)
devtools::install_github('hadley/ggplot2')
qmap(location = "boston university")
library("ggforce", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("ggmap", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("ggraph", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("ggrepel", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
qmap(location = "boston university")
devtools::install_github('hadley/ggplot2', force=TRUE)
devtools::install_github('thomasp85/ggforce',force=TRUE)
qmap(location = "baylor university")
devtools::install_github("dkahle/ggmap")
qmap(location = "boston university")
qmap(location = "baylor university")
us <- c(left = -125, bottom = 25.75, right = -67, top = 49)
map <- get_stamenmap(us, zoom = 5, maptype = "toner-lite")
ggmap(map)
detach("package:ggmap", unload=TRUE)
library("ggmap", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
qmap(location = "boston university")
devtools::install_github("dkahle/ggmap",force=TRUE)
devtools::install_github('hadley/ggplot2', force=TRUE)
devtools::install_github('thomasp85/ggforce',force=TRUE)
devtools::install_github('thomasp85/ggraph',force=TRUE)
devtools::install_github('slowkow/ggrepel',force=TRUE)
detach("package:ggforce", unload=TRUE)
detach("package:ggmap", unload=TRUE)
detach("package:ggplot2", unload=TRUE)
detach("package:ggraph", unload=TRUE)
detach("package:ggrepel", unload=TRUE)
library("ggmap", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("ggraph", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("ggrepel", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
qmap(location = "boston university")
get_googlemap("waco texas", zoom = 12, maptype = "hybrid") %>% ggmap()
install.packages("RgoogleMaps")
library("RgoogleMaps", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
bb <- qbbox(c(40.702147,40.711614,40.718217),
+   c(-74.015794,-74.012318,-73.998284), TYPE = "all",
+   margin = list(m=rep(5,4), TYPE = c("perc", "abs")[1]));
bb <- qbbox(c(40.702147,40.711614,40.718217),
c(-74.015794,-74.012318,-73.998284), TYPE = "all",
margin = list(m=rep(5,4), TYPE = c("perc", "abs")[1]));
map2= GetMap.bbox(bb$lonR, bb$latR,destfile = "MyTile3.png",
maptype = "satellite", size=c(640,640))
PlotOnStaticMap(map2)
lat = c(40.702147,40.718217,40.711614);
lon = c(-74.012318,-74.015794,-73.998284);
center = c(mean(lat), mean(lon));
zoom <- min(MaxZoom(range(lat), range(lon)));
markers = paste0("&markers=color:blue|label:S|40.714511,-74.009684&markers=color:", "green|label:G|40.714511,-74.009684&markers=color:red|color:red|")
ClubMap <- GetMap(center=center, zoom=zoom,markers=markers,destfile = "Dolls.png");
PlotOnStaticMap(ClubMap)
devtools::install_github("dkahle/ggmap",force=TRUE)
install_github("hadley/ggplot2@v2.2.0", force=TRUE)
devtools::install_github("hadley/ggplot2@v2.2.0", force=TRUE)
devtools::install_github('thomasp85/ggraph',force=TRUE)
devtools::install_github('slowkow/ggrepel',force=TRUE)
library("ggmap", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("ggraph", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("ggrepel", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
qmap(location = "boston university")
detach("package:ggmap", unload=TRUE)
detach("package:ggplot2", unload=TRUE)
library("ggmap", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
detach("package:ggmap", unload=TRUE)
detach("package:ggraph", unload=TRUE)
detach("package:ggrepel", unload=TRUE)
library('ggmap')
library('ggplot2')
library('ggraph')
library('ggrepel')
library('ggplot2')
library('ggplot')
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("ggplot2")
qmap(location = "baylor university")
get_googlemap("waco texas", zoom = 12, maptype = "hybrid") %>% ggmap()
qmap(location = "boston university",zoom = 12)
qmap(location = "boston university",zoom = 30)
qmap(location = "boston university",zoom = 25)
qmap(location = "boston university",zoom = 20)
qmap(location = "boston university",zoom = 15)
qmap(location = "boston university",zoom = 16)
qmap(location = "boston university",zoom = 17)
qmap(location = "baylor university")
us <- c(left = -125, bottom = 25.75, right = -67, top = 49)
map <- get_stamenmap(us, zoom = 5, maptype = "toner-lite")
ggmap(map)
library(dplyr)
crime
violent_crimes <- filter(crime,
offense != "auto theft", offense != "theft", offense != "burglary"
)
violent_crimes$offense <- factor(
violent_crimes$offense,
levels = c("robbery", "aggravated assault", "rape", "murder")
)
violent_crimes <- filter(violent_crimes,
-95.39681 <= lon & lon <= -95.34188,
29.73631 <= lat & lat <=  29.78400
)
qmplot(lon, lat, data = violent_crimes, maptype = "toner-lite", color = I("red"))
robberies <- violent_crimes %>% filter(offense == "robbery")
qmplot(lon, lat, data = violent_crimes, geom = "blank", zoom = 15, maptype = "toner-background", darken = .7, legend = "topleft") +
stat_density_2d(aes(fill = ..level..), geom = "polygon", alpha = .3, color = NA) +
scale_fill_gradient2("Robbery\nPropensity", low = "white", mid = "yellow", high = "red", midpoint = 650)
qmplot(lon, lat, data = violent_crimes, maptype = "toner-background", color = offense) +
facet_wrap(~ offense)
qmap(location = "Rockhurst university")
qmap(location = "boston university",zoom = 17, maptype = "hybrid")
qmap(location = "12710 S. Hagan, 66062",zoom = 17, maptype = "hybrid")
qmap(location = "12710 S. Hagan, 66062",zoom = 20, maptype = "hybrid")
View(robberies)
install.packages("gcookbook")
library(gcookbook) # For the data set
library(igraph)
#look at the data
madmen2
g <- graph.data.frame(madmen2, directed=TRUE)
par(mar=c(0,0,0,0))
plot(g, layout=layout.fruchterman.reingold, vertex.size=8, edge.arrow.size=0.5,
vertex.label=NA)
library(igraph)
library(gcookbook) # For the data set
# Copy madmen and drop every other row
m <- madmen[1:nrow(madmen) %% 2 == 1, ]
g <- graph.data.frame(m, directed=FALSE)
V(g)$name
plot(g, layout=layout.fruchterman.reingold,  #l.f.r is a popular SN algorythym
vertex.size        = 4,          # Smaller nodes
vertex.label       = V(g)$name,  # Set the labels
vertex.label.cex   = 0.8,        # Slightly smaller font
vertex.label.dist  = 0.4,        # Offset the labels
vertex.label.color = "black")
g$layout <- layout.fruchterman.reingold  #l.f.r is a popular SN algorythym
plot(g)
E(g)
# Set some of the labels to "M"
E(g)[c(2,11,19)]$label <- "M"
# Set color of all to grey, and then color a few red
E(g)$color             <- "grey70"
E(g)[c(2,11,19)]$color <- "red"
plot(g)
install.packages('network')
library(network)
nodeinfo <- read.table("http://www.stat.psu.edu/~dhunter/Rnetworks/nodal.attr.txt", head=T)
myedges <- read.table("http://www.stat.psu.edu/~dhunter/Rnetworks/edgelist.txt")
diseasenw <- network(myedges, directed=F, bipartite=132,
vertex.attr = nodeinfo)
plot(diseasenw, vertex.col=3-nodeinfo$race,
vertex.sides=2+nodeinfo$sex,
main="Squares are female; triangles are male")
data(flo)
nflo<-network(flo)
plot(nflo, displaylabels=TRUE,vertex.cex=apply(flo,2,sum)+1, usearrows=FALSE,
vertex.sides=3+apply(flo,2,sum),
vertex.col=2+(network.vertex.names(nflo)=="Medici"))
install.packages("devtools")
require(devtools)
install.packages("choroplethr")
api.key.install(key="2e254c3c4a1ad6fe3322b0a267446163148e7a78")
choroplethr_acs(tableId="B19301", lod="state")
nba <- read.csv("http://datasets.flowingdata.com/ppg2008.csv", sep=",")
View(nba)
nba <- nba[order(nba$PTS),]
row.names(nba) <- nba$Name
nba <- nba[,2:20]
nba_matrix <- data.matrix(nba)
nba_heatmap <- heatmap(nba_matrix, Rowv=NA, Colv=NA, col = cm.colors(256), scale="column", margins=c(5,10))
playerID <- 201939
shotURL <- paste("http://stats.nba.com/stats/shotchartdetail?CFID=33&CFPARAMS=2014-15&ContextFilter=&ContextMeasure=FGA&DateFrom=&DateTo=&GameID=&GameSegment=&LastNGames=0&LeagueID=00&Location=&MeasureType=Base&Month=0&OpponentTeamID=0&Outcome=&PaceAdjust=N&PerMode=PerGame&Period=0&PlayerID=",playerID,"&PlusMinus=N&Position=&Rank=N&RookieYear=&Season=2014-15&SeasonSegment=&SeasonType=Regular+Season&TeamID=0&VsConference=&VsDivision=&mode=Advanced&showDetails=0&showShots=1&showZones=0", sep = "")
shotData <- fromJSON(file = shotURL, method="C")
install.packages("rjson")
library(rjson)
playerID <- 201939
shotURL <- paste("http://stats.nba.com/stats/shotchartdetail?CFID=33&CFPARAMS=2014-15&ContextFilter=&ContextMeasure=FGA&DateFrom=&DateTo=&GameID=&GameSegment=&LastNGames=0&LeagueID=00&Location=&MeasureType=Base&Month=0&OpponentTeamID=0&Outcome=&PaceAdjust=N&PerMode=PerGame&Period=0&PlayerID=",playerID,"&PlusMinus=N&Position=&Rank=N&RookieYear=&Season=2014-15&SeasonSegment=&SeasonType=Regular+Season&TeamID=0&VsConference=&VsDivision=&mode=Advanced&showDetails=0&showShots=1&showZones=0", sep = "")
shotData <- fromJSON(file = shotURL, method="C")
install.packages(c("choroplethr", "choroplethrAdmin1"))
library(choroplethr)
library(choroplethrAdmin1)
data(df_japan_census)
View(df_japan_census)
head(df_japan_census)
df_japan_census$value = df_japan_census$pop_density_km2_2010
admin1_choropleth("japan", df_japan_census)
admin1_choropleth("japan", df_japan_census, num_colors=1, reference_map=TRUE)
data(df_pop_county)
county_choropleth(df_pop_county,
title      = "US 2012 County Population Estimates",
legend     = "Population",
num_colors = 1,
state_zoom = c("california", "oregon", "washington"))
install.packages(c("choroplethr", "choroplethrAdmin1", "choroplethrMaps"))
install.packages(c("choroplethr", "choroplethrAdmin1", "choroplethrMaps"))
library("choroplethr", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("choroplethrAdmin1", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("choroplethrMaps", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
data(df_pop_county)
county_choropleth(df_pop_county,
title      = "US 2012 County Population Estimates",
legend     = "Population",
num_colors = 1,
state_zoom = c("california", "oregon", "washington"))
setwd('/Users/mpgartland/Documents/Courses/Predictive Models/Pred_Models_git/Week 5/data')
concrete <- read.csv("concrete.csv")
str(concrete)
concrete <- read.csv("concrete.csv")
str(concrete)
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# apply min/max normalization to entire data frame
#note all values are now between 0 and 1
concrete_norm <- as.data.frame(lapply(concrete, normalize))
boxplot(concrete_norm)
# confirm that the range is now between zero and one
summary(concrete_norm$strength)
# compared to the original minimum and maximum
summary(concrete$strength)
# create training and test data
#Split the dataset into a training and testing sets 70/30
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
## Step 3: Training a model on the data ----
# train the neuralnet model
library(neuralnet)
install.packages("neuralnet")
library(neuralnet)
concrete_model_2 <- neuralnet(formula = strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train, hidden = 2, algorithm = "rprop+", learningrate=NULL)
#
plot(concrete_model_2)
concrete_model_2$result.matrix
## Step 4: Evaluating model performance ----
# obtain model results
model_results_2 <- compute(concrete_model_2, concrete_test[1:8]) #You are running the training set through the ANN model
# obtain predicted strength values
predicted_strength_2 <- model_results_2$net.result #The prediction of each observation
# examine the correlation between predicted and actual values
cor(predicted_strength_2, concrete_test$strength)
#RMSE
sqrt(mean((concrete_test$strength-predicted_strength_2)^2))
setwd('/Users/mpgartland/Documents/Courses/Predictive Models/Pred_Models_git/Week 5/data')
## Step 2: Exploring and preparing the data ----
# read in data and examine structure
concrete <- read.csv("concrete.csv")
str(concrete)
#custom normalization function
#This is called min/max normalization (vs z-score)
#Normalization by Scaling Between 0 and 1
#Common way for ANN
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# apply min/max normalization to entire data frame
#note all values are now between 0 and 1
concrete_norm <- as.data.frame(lapply(concrete, normalize))
boxplot(concrete_norm)
# confirm that the range is now between zero and one
summary(concrete_norm$strength)
# compared to the original minimum and maximum
summary(concrete$strength)
# create training and test data
#Split the dataset into a training and testing sets 70/30
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
## Step 3: Training a model on the data ----
# train the neuralnet model
library(neuralnet)
# simple ANN with only a two hidden neurons
concrete_model_1 <- neuralnet(formula = strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train, hidden = 2, algorithm = "rprop+", learningrate=NULL)
#rprop+ is a backpropagation method called resilient backpropagation. It modifies
#its learning rate on the error.
# visualize the network topology
#note one node in the hidden layer
plot(concrete_model_1)
#table of nuerons and weights
concrete_model_2$result.matrix
## Step 4: Evaluating model performance ----
# obtain model results
model_results_1 <- compute(concrete_model_1, concrete_test[1:8]) #You are running the training set through the ANN model
# obtain predicted strength values
predicted_strength_1 <- model_results_1$net.result #The prediction of each observation
# examine the correlation between predicted and actual values
cor(predicted_strength_1, concrete_test$strength)
#RMSE
sqrt(mean((concrete_test$strength-predicted_strength_1)^2))
setwd('/Users/mpgartland/Documents/Courses/Predictive Models/Pred_Models_git/Week 5/data')
## Step 2: Exploring and preparing the data ----
# read in data and examine structure
concrete <- read.csv("concrete.csv")
str(concrete)
#custom normalization function
#This is called min/max normalization (vs z-score)
#Normalization by Scaling Between 0 and 1
#Common way for ANN
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# apply min/max normalization to entire data frame
#note all values are now between 0 and 1
concrete_norm <- as.data.frame(lapply(concrete, normalize))
boxplot(concrete_norm)
# confirm that the range is now between zero and one
summary(concrete_norm$strength)
# compared to the original minimum and maximum
summary(concrete$strength)
# create training and test data
#Split the dataset into a training and testing sets 70/30
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
## Step 3: Training a model on the data ----
# train the neuralnet model
library(neuralnet)
# simple ANN with only a two hidden neurons
concrete_model_1 <- neuralnet(formula = strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train, hidden = 2, algorithm = "rprop+", learningrate=NULL)
#rprop+ is a backpropagation method called resilient backpropagation. It modifies
#its learning rate on the error.
# visualize the network topology
#note one node in the hidden layer
plot(concrete_model_1)
#table of nuerons and weights
concrete_model_1$result.matrix
## Step 4: Evaluating model performance ----
# obtain model results
model_results_1 <- compute(concrete_model_1, concrete_test[1:8]) #You are running the training set through the ANN model
# obtain predicted strength values
predicted_strength_1 <- model_results_1$net.result #The prediction of each observation
# examine the correlation between predicted and actual values
cor(predicted_strength_1, concrete_test$strength)
#RMSE
sqrt(mean((concrete_test$strength-predicted_strength_1)^2))
concrete_model2 <- neuralnet(strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train, hidden = 5,algorithm = "rprop+", learningrate=NULL)
# plot the network
#note 5 neurons in the hidden layer
plot(concrete_model2)
# evaluate the results as we did before
model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
predicted_strength2[1:10]
#what do you notice about the values?
#Return norm value to a regular value
denormalize <- function(x) {
return(x*(max(concrete$strength)) - min(concrete$strength))+min(concrete$strength)
}
#look at predicted vs actual
accuracy<-data.frame(denormalize(predicted_strength2),concrete$strength[774:1030])
#plot pred vs actual
plot(denormalize(predicted_strength2),concrete$strength[774:1030])
concrete_model3 <- neuralnet(strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train, hidden = c(5,3), algorithm = "rprop+", learningrate=NULL)
plot(concrete_model3)
set.seed(1234567890)
library("neuralnet")
setwd('/Users/mpgartland/Documents/Courses/Predictive Models/Pred_Models_git/Week 5/data')
creditset <- read.csv("creditset.csv")
head(creditset)
#Common way for ANN
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# apply min/max normalization to entire data frame
#note all values are now between 0 and 1
creditset_norm <- as.data.frame(lapply(creditset, normalize))
## extract a set to train the NN
trainset <- creditset_norm[1:800, ]
## select the test set
testset <- creditset_norm[801:2000, ]
#ANN with two inputs and 1 output (2 class target)
creditnet <- neuralnet(default10yr ~ LTI + age +income+ loan, trainset, hidden = 4, lifesign = "minimal",
linear.output = FALSE, threshold = 0.1)
plot(creditnet, rep = "best")
## test the resulting output
#temp_test <- subset(testset, select = c("LTI", "age",))
creditnet.results <- compute(creditnet, testset)
results <- data.frame(actual = testset$default10yr, prediction = creditnet.results$net.result)
results[100:115, ]
setwd('//Users/mpgartland/Documents/Courses/Predictive Models/Pred_Models_git/Week 5/data')
library("neuralnet")
# you can also see the packages nnet and RSNNS
dataset <- read.csv("creditset.csv")
head(dataset)
## extract a set to train the NN @ 65%
trainset <- dataset[1:1300, ]
## select the test set
testset <- dataset[1301:2000, ]
#NN nodel with one hiddeD layer and 4 nodes, resilient backprop
creditnet <- neuralnet(default10yr ~ LTI +age, trainset, hidden = 4, lifesign = "minimal",
linear.output = FALSE, threshold = 0.1,algorithm = "rprop+")
print(creditnet)
## plot the NN
plot(creditnet, rep = "best")
#weight from models
creditnet$weights
creditnet$result.matrix
#results (notice in probabilities)
creditnet$net.result
## test the resulting output
temp_test <- subset(testset, select = c("LTI", "age"))
#compute the test results through the ANN model
creditnet.results <- compute(creditnet, temp_test)
#Make a table to predictions and actuals
results <- data.frame(actual = testset$default10yr,
prediction = creditnet.results$net.result)
print(results)
#Round predictions to make easier to read
results$prediction <- round(results$prediction)
#See partial results
results[100:115, ]
#See all results
print(results)
print(creditnet.results)
#USing Gmodels to create a confusion matrix
library(gmodels)
CrossTable(results$actual, results$prediction)
install.packages("gmodels")
library(gmodels)
CrossTable(results$actual, results$prediction)
setwd('//Users/mpgartland/Documents/Courses/Predictive Models/Pred_Models_git/Week 5/data')
library("neuralnet")
# you can also see the packages nnet and RSNNS
dataset <- read.csv("creditset.csv")
head(dataset)
## extract a set to train the NN @ 65%
trainset <- dataset[1:1300, ]
## select the test set
testset <- dataset[1301:2000, ]
#NN nodel with one hiddeD layer and 4 nodes, resilient backprop
creditnet <- neuralnet(default10yr ~ LTI +age, trainset, hidden = 4, lifesign = "minimal",
linear.output = FALSE, threshold = 0.1,algorithm = "rprop+")
print(creditnet)
## plot the NN
plot(creditnet, rep = "best")
#weight from models
creditnet$weights
creditnet$result.matrix
#results (notice in probabilities)
creditnet$net.result
## test the resulting output
temp_test <- subset(testset, select = c("LTI", "age"))
#compute the test results through the ANN model
creditnet.results <- compute(creditnet, temp_test)
#Make a table to predictions and actuals
results <- data.frame(actual = testset$default10yr,
prediction = creditnet.results$net.result)
print(results)
#Round predictions to make easier to read
results$prediction <- round(results$prediction)
#See partial results
results[100:115, ]
#See all results
print(results)
print(creditnet.results)
#USing Gmodels to create a confusion matrix
library(gmodels)
CrossTable(results$actual, results$prediction)
